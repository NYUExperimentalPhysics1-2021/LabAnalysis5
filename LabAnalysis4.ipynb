{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LabAnalysis4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIqel/7qq5pMjnHiq9o6TQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NYUExperimentalPhysics1-2021/LabAnalysis4/blob/main/LabAnalysis4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpswBbfWcR0E"
      },
      "source": [
        "#Initial library includes and installations\n",
        "run once - does not require you to edit anything"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHTWtceOIVdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e99a179-5299-47d7-df99-8f0c6aac7869"
      },
      "source": [
        "!pip install munch\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, glob\n",
        "from munch import munchify\n",
        "import scipy.stats\n",
        "from sklearn import linear_model, datasets\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch) (1.15.0)\n",
            "Installing collected packages: munch\n",
            "Successfully installed munch-2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGierwNjcdQa"
      },
      "source": [
        "#Function definitions and constants\n",
        "Run once - does not require you to edit anything\n",
        "\n",
        "These functions are provided for you - see function definitions and comments for more information on their return values and usage\n",
        "\n",
        "  1. `loadDataSet(filename)` - loads an individual .json file and checks it for really large jumps in major axis angle, which would indicate a problem with the fits\n",
        "  1. `loadAllDataSets(startdir)` - loads all json files in a directory\n",
        "  1. `meanOverTime(t,y,deltat)` - breaks data up into chunks about delta t long and calculates the average value of y over each chunk (assumes t is evenly spaced and monotonically increasing)\n",
        "  1. `rateOfChange (t,y)` - calculates dy/dt (assumes t is monotonically increasing)\n",
        "  1. `rateOfChangeAngular (t,theta)` - calculates dtheta/dt, taking into account wrapping every 2$\\pi$ (assumes t is monotonically increasing and theta is in **radians**)\n",
        "  1. `(m,b,m_e,b_e) fitLine(x,y)` - least squares fit to $y = (m\\pm m_e)x + b\\pm b_e$ $m_e$ and $b_e$ are the uncertainties (errors) in the estimates of $m$ and $b$\n",
        "  1. `fitLineRansac(x,y)` - least squares fit to y = m x + b , discarding outliers - note that this function returns extra values besides m, b, m_e nd b_e: inlying and outlying points : uses [RANSAC](https://https://en.wikipedia.org/wiki/Random_sample_consensus) to find outliers\n",
        "  1. `fitLineHuber(x,y)` - least squares fit to y = m x + b , discarding outliers - note that this function returns extra values besides m, b, m_e nd b_e: inlying and outlying points : uses [Huber](https://scikit-learn.org/stable/modules/linear_model.html#huber-regression) to find outliers\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3wquqyKbOpM"
      },
      "source": [
        "gaccel = 9.802\n",
        "\n",
        "nyc_latitude =40.730610\n",
        "deghr = np.rad2deg(3600)\n",
        "omega_foucault = -15*np.sin(np.deg2rad(nyc_latitude))/deghr\n",
        "\n",
        "\n",
        "MAJAXIS = 0\n",
        "MINAXIS = 1\n",
        "THETA = 2\n",
        "DPHI = 3\n",
        "MAGROT = 4\n",
        "MAGROTRATE =5\n",
        "\n",
        "\n",
        "#loads one data set (work from one lab group stored in a .json file)\n",
        "#data can be accessed as either a dictionary or a structure\n",
        "#ie dataset[\"fits\"] and dataset.fits are the same thing\n",
        "#setupNumber: number on the wall\n",
        "#section: 1-4 which section data came from\n",
        "#thetaWall: angle of a line parallel to the wall\n",
        "#z0: calibrated magnet height\n",
        "#filename: name of the json file\n",
        "#fits: list of fits to individual trials\n",
        "#   anglePath: path to angle data file\n",
        "#   magPath: path to magnetometer data file\n",
        "#   offset: (x,y) calculated offset of the magnetometer from 0\n",
        "#   tiltAngle: calculated angle the magnet was tilted away from z-axis\n",
        "#   L: calculated length of the string (from period)\n",
        "#   B0: best estimate of magnetic field strength z0 away directly along magnet axis\n",
        "#   t: time each orbit started: (N,)\n",
        "#   orbit: fit parameters for each orbit (N,6)\n",
        "# >>    orbit[:,MAJAXIS] is the major axis size (in meters) <<<\n",
        "# >>    orbit[:,MINAXIS] is the minor axis size (in meters) <<<\n",
        "# >>    orbit[:,THETA] is the major axis angle <<<\n",
        "#       orbit[:,DPHI] is the phase offset\n",
        "#       orbit[:,MAGROT] is the orientation of the magnet tilt relative to the x-axis\n",
        "#       orbit[:,MAGROTRATE] is the rate the bob was spinning about its axis per period\n",
        "\n",
        "\n",
        "def loadDataSet(filename):\n",
        "  file = open(filename,'r')\n",
        "  results = json.load(file)\n",
        "  file.close\n",
        "  dataset = munchify(results) #can be accessed as a structure or a dict\n",
        "  dataset.filename = filename\n",
        "  valid = []\n",
        "  for f in dataset.fits:\n",
        "    f.offset = np.array(f.offset)\n",
        "    f.orbit = np.array(f.orbit)\n",
        "    f.t = np.array(f.t) \n",
        "    dp = np.diff(np.unwrap(f.orbit[:,THETA]))\n",
        "    valid.append((np.abs(dp) < np.pi/4).all()) #extremely large jump from one swing to the next - bad fit\n",
        "  bad = np.array(valid) == False\n",
        "  if (bad.any()):\n",
        "    print('{}: bad fits found in experiments {}'.format(filename, np.where(bad)[0]))\n",
        "    dataset.fits = [dataset.fits[i] for i in np.where(valid)[0]]\n",
        "  return dataset\n",
        "\n",
        "def loadAllDataSets(startdir):\n",
        "  files = sorted(glob.glob(startdir + '/*.json'))\n",
        "  return [loadDataSet(f) for f in files]\n",
        "\n",
        "#my = meanOverTime (t,y,deltat)\n",
        "#calculates the mean value of y and dy/dt at approximate intervals of deltat\n",
        "#   e.g. if deltat is 10, then my[0] is the average value of y between \n",
        "#   t[0] and t[0] + 10\n",
        "#detalt is adjusted downward to evenly divide the whole range\n",
        "#   e.g. if deltat is 100 and t ranges from 0 to 110 seconds, then delta t will be 55\n",
        "def meanOverTime (t, y, deltat):\n",
        "  numpts = int(np.ceil((t[-1]-t[0])/deltat)) + 1\n",
        "  inds = np.linspace(0,len(t),numpts, endpoint=False, dtype=int)\n",
        "  dt = np.gradient(t)\n",
        "  my = np.diff((np.cumsum(y)*dt)[inds])/np.diff(t[inds])\n",
        "  return my\n",
        "\n",
        "#dy_dt = rateOfChange(t,y)  \n",
        "def rateOfChange (t,y):\n",
        "  return np.gradient(y)/np.gradient(t)\n",
        "\n",
        "#dtheta_dt = rateOfChangeAngular(theta)\n",
        "#unwraps theta so that there aren't any jumps bigger than pi before taking derivative\n",
        "def rateOfChangeAngular(t,theta):\n",
        "  return np.gradient(np.unwrap(theta))/np.gradient(t)\n",
        "\n",
        "#(m,b,m_e,b_e) = fitLine(x,y)\n",
        "#least squares fit to y = m x + b : m_e,b_e are uncertainties in m,b\n",
        "def fitLine(x,y):\n",
        "  p = np.polyfit(x,y,1)\n",
        "  res = y - p[0]*x - p[1]\n",
        "  m_e = np.sqrt(np.var(res)/np.sum((x-np.mean(x))**2))\n",
        "  b_e = np.sqrt(np.mean(x**2))*m_e\n",
        "  return (p[0],p[1],m_e,b_e)\n",
        "  \n",
        "\n",
        "def fitLineHuber(x,y,epsilon=2):\n",
        "  huber = linear_model.HuberRegressor(epsilon=epsilon)\n",
        "  huber.fit(x.reshape(-1,1),y)\n",
        "  outlier_mask = huber.outliers_\n",
        "  inlier_mask = np.logical_not(outlier_mask)\n",
        "  (m,b,me,be) = fitLine(x[inlier_mask], y[inlier_mask])\n",
        "  xi = x[inlier_mask]\n",
        "  yi = y[inlier_mask]\n",
        "  xo = x[outlier_mask]\n",
        "  yo = y[outlier_mask]\n",
        "  return (m,b,me,be,xi,yi,xo,yo)\n",
        "\n",
        "#((m,b,me,be,xi,yi,xo,yo) = fitLineRansac(x,y)\n",
        "#least squares fit to y = m x + b , discarding outliers\n",
        "#xi,yi are x,y values used (inliers) \n",
        "#xo.yo are x,y values discarded (outliers)\n",
        "def fitLineRansac(x,y):\n",
        "  #print(np.median(np.abs(y - np.median(y))))\n",
        "  ransac = linear_model.RANSACRegressor()\n",
        "  ransac.fit(x.reshape(-1,1),y.reshape(-1,1))\n",
        "  inlier_mask = ransac.inlier_mask_\n",
        "  res = y[inlier_mask]-ransac.predict(x.reshape(-1,1))[inlier_mask,0]\n",
        "  ransac.residual_threshold = np.median(np.abs(res))\n",
        "  ransac.fit(x.reshape(-1,1),y.reshape(-1,1))\n",
        "\n",
        "  ransac.stop_probability = 1\n",
        "  ransac.max_trials = 10000\n",
        "  ransac.min_samples = 0.1\n",
        "  res = y[inlier_mask]-ransac.predict(x.reshape(-1,1))[inlier_mask,0]\n",
        "  ransac.residual_threshold = 2.5*np.std(res)\n",
        "  ransac.fit(x.reshape(-1,1),y.reshape(-1,1))\n",
        "  inlier_mask = ransac.inlier_mask_\n",
        "  \n",
        "  # res = y[inlier_mask]-ransac.predict(x.reshape(-1,1))[inlier_mask,0]\n",
        "  # ransac.residual_threshold = np.median(np.abs(res))\n",
        "  # ransac.fit(x.reshape(-1,1),y.reshape(-1,1))\n",
        "  # inlier_mask = ransac.inlier_mask_\n",
        "\n",
        "  outlier_mask = np.logical_not(inlier_mask)\n",
        "  (m,b,me,be) = fitLine(x[inlier_mask], y[inlier_mask])\n",
        "  xi = x[inlier_mask]\n",
        "  yi = y[inlier_mask]\n",
        "  xo = x[outlier_mask]\n",
        "  yo = y[outlier_mask]\n",
        "  print(ransac.n_trials_)\n",
        "  return (m,b,me,be,xi,yi,xo,yo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRZ82mnQeKBL"
      },
      "source": [
        "#Fetch the data to be analyzed\n",
        "** Change the first github repository name to match your username (lab4-individual-data-yourname)  **\n",
        "then run once"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCynEIfRabj6",
        "outputId": "5a06d1f3-3d0c-4326-de93-9a9971034560"
      },
      "source": [
        "!rm -rf mydata/\n",
        "!rm -rf data/\n",
        "!git clone https://github.com/NYUExperimentalPhysics1-2021/lab4-individual-data-mgershow mydata #change to your github username\n",
        "!git clone https://github.com/NYUExperimentalPhysics1-2021/lab4-shared-data-whole-class data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mydata'...\n",
            "remote: Enumerating objects: 51, done.\u001b[K\n",
            "remote: Counting objects: 100% (51/51), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 51 (delta 1), reused 46 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (51/51), done.\n",
            "Checking out files: 100% (44/44), done.\n",
            "Cloning into 'data'...\n",
            "remote: Enumerating objects: 64, done.\u001b[K\n",
            "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 64 (delta 19), reused 7 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (64/64), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChmLWszotrxC"
      },
      "source": [
        "#load the data sets to be analyzed\n",
        "run once\n",
        "\n",
        "1. `mydata = loadAllDataSets('/content/mydata/')[0]` - load your data -- the [0] is because you only have one data set, so we'll take it out of the list to look at it alone\n",
        "1. `alldata = loadAllDataSets('/content/data/')` - load everyone's data - each element of this list is one dataset\n",
        "\n",
        "\n",
        "## fields in dataset structure\n",
        "- `setupNumber`: number on the wall\n",
        "- `section`: 1-4 which section data came from\n",
        "- `thetaWall`: angle of a line parallel to the wall\n",
        "- `z0`: calibrated magnet height\n",
        "- `filename`: name of the json file\n",
        "- `fits`: list of fits to individual trials\n",
        "   - `anglePath`: path to angle data file\n",
        "   - `magPath`: path to magnetometer data file\n",
        "   - `offset`: (x,y) calculated offset of the magnetometer from 0\n",
        "   - `tiltAngle`: calculated angle the magnet was tilted away from z-axis\n",
        "   - `L`: calculated length of the string (from period)\n",
        "   - `B0`: best estimate of magnetic field strength z0 away directly along magnet axis\n",
        "   - `t`: time each orbit started: (N,)\n",
        "   - `orbit`: fit parameters for each orbit (N,6)\n",
        "        - `orbit[:,MAJAXIS]` is the **major axis** size (in meters) \n",
        "        - `orbit[:,MINAXIS]` is the **minor axis** size (in meters) \n",
        "        - `orbit[:,THETA]` is the **angle of the major axis** \n",
        "        - `orbit[:,DPHI]` is the phase offset\n",
        "        - `orbit[:,MAGROT]` is the orientation of the magnet tilt relative to the x-axis\n",
        "        - `orbit[:,MAGROTRATE]` is the rate the bob was spinning about its axis per period"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXOAJtsGazrU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8a80c8d-f8b3-440d-c27e-cf1fdfdfa25d"
      },
      "source": [
        "mydata = loadAllDataSets('/content/mydata/')[0]\n",
        "alldata = loadAllDataSets('/content/data/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data/section3setup1-16-Nov-2021.json: bad fits found in experiments [0]\n",
            "/content/data/section3setup6-16-Nov-2021.json: bad fits found in experiments [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XAUhoOivZri"
      },
      "source": [
        "#Part 1 - plot the precession rate vs. the predicted precession rate for your experiments\n",
        "\n",
        "1. Choose a time bin size, $\\Delta t$ to average over. Start with `deltat = 60` seconds, but try a few different values once everything works.\n",
        "1. Create empty lists to store the following results: `predictedrate`, `measuredrate`\n",
        "1. Loop over each fit (trial) in your set (`for f in mydata.fits:`)\n",
        "    1. Calculate $\\omega = \\sqrt{g/L}$ Note that $g$ is defined in the variable `gaccel` and $L$ can be found in `f.L`\n",
        "    1. Extract the time, major axis ($a$), minor axis ($b$), and angle ($\\theta$) of the swing. e.g. `t = f.t` for time; `a = f.orbit[:,MAJAXIS]` for major axis, etc.\n",
        "    1. Calculate the predicted precession rate (without foucault precession):  $\\Omega_{pred} = 0.375 * \\omega * \\frac{a b}{L^2}$\n",
        "    1. Calculate the measured precession rate as $\\frac{d \\theta}{d t}$. Make sure to use `rateOfChangeAngular`\n",
        "    1. For both the predicted and measured rates, calculate the means over time, using `meanOverTime` with your stored value of `deltat`\n",
        "    1. Append the mean-over-time predicted and measured rates to predictedrate and measuredrate. \n",
        "    1. Use `plt.scatter(pred*deghr,meas*deghr)` to make a scatter plot of the results of this trial. Here pred and meas are the mean-over-time rates you just calculated. Plotting the data one trial at a time should result in each trial showing up as different colored dots. If the dots are too large, add `marker='.'` after the arguments of the scatter command\n",
        "\n",
        "1. Convert your lists of fit values to a single array: `pred = np.concatenate(predictedrate)`, `meas = np.concatenate(measuredrate)`\n",
        "1. Plot the prediction of the model and foucault precession: `plt.plot(pred*deghr, (pred + omega_foucault)*deghr, 'm--', label = 'predicted')` as a dashed magenta line\n",
        "1. Fit the measured rate to the predicted rate using `fitLine` and plot the prediction as a black solid line. Make sure to multiply by `deghr`. Add `label = 'measured'` to the command. \n",
        "1. Use `plt.legend()` to make a legend\n",
        "1. Add x and y labels. Here's how I made my x-label `plt.xlabel(r'predicted: $\\frac{3}{8}\\omega \\frac{a b}{L^2}$ (deg/hr)')`\n",
        "1. Print out the following information: slope of the linear fit between predicted (x) and measured (y) values (would be 1 if measured rates exactly match predicted rates); 2 intercept of the linear fit, multiplied by deghr (would be -9.8 deg/hr if only perturbing influence is foucault precession)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmcUb4LWbdjM"
      },
      "source": [
        "## your code below\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq_NsZ0N7gjg"
      },
      "source": [
        "#Questions\n",
        "\n",
        "1. To what extent are your data consistent with the theory? Do you see qualitative agreement (i.e. the direction matches the predicted direction and depends on the size of $ab/L^2$)? Is the measured rate linear in $ab/L^2$ ? Are the slope and intercepts what you would expect?\n",
        "\n",
        "1. Let's think about the intercept a little more. Say you can measure the change in angle with an accuracy of 1 degree over your 5 minute experiment.\n",
        "    1. What accuracy is this in terms of degrees per hour? \n",
        "    1. When we calculate s.e.m., $\\sigma_{mean} = \\frac{\\sigma}{\\sqrt{N}}$. The same rule will apply to the error in the intercept ($\\sigma_{intercept} = \\frac{\\sigma}{\\sqrt{N}}$), if the mean of the x-data is 0. What error in the intercept would you expect for the number of measurements you made?\n",
        "    1. How does this compare to the actual error reported by fitLine?\n",
        "    1. How many 5-minute experiments would you need to get $\\sigma_{intercept}$ down to 1 degree/hour?\n",
        "\n",
        "1. If you didn't already, play with `deltat` (try 10, 100 and 300 - don't go below 3 seconds or you might see an error if a time interval has no crossings) and examine how the graph and the fit change. How does a longer `deltat` change the accuracy of your measured rate? Setting `deltat` to 10,000 will make the software treat every trial as a single measurement. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbAVJ5e9D-c1"
      },
      "source": [
        "#Everyone's data\n",
        "Now let's apply the same procedure to look at the whole class data set. You should be able to basically reuse your code above. Just wrap everything in another loop\n",
        "\n",
        "```python\n",
        "ratepred = []\n",
        "ratemeas = []\n",
        "deltat = 100\n",
        "for data in alldata:\n",
        "  for f in data.fits:\n",
        "    ....\n",
        "    ....\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-riQyc2hM_wW"
      },
      "source": [
        "#your code below\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PblN7P8Iacdg"
      },
      "source": [
        "#Deal with possible outliers\n",
        "\n",
        "If you look at the class data, you can see the line is being \"pulled up\" by some measurements with very large predicted and measured rates.\n",
        "\n",
        "1. One way to deal with this would be to focus only on a smaller range. IE analyze the data where the predicted rate is between -200 and 200 deg/hr. \n",
        "1. There are also robust fitting strategies you can use. I've written function `fitLineRansac` and `fitLineHuber` that use these to identify \"outliers\" - data you discard as unlikely to be correctly measured/part of the same data set. \n",
        "\n",
        "Please...\n",
        "\n",
        "1. Plot all the data as a scatter plot overlaid with the linear fit in black and the prediction as a dashed magenta line (slope = 1, intercept = -9.8 deg/hr)\n",
        "1. Make a second figure. Divide the data by whether the absolute value of the predicted rate is more or less than 200 deg/hr. Plot the \"inliers\" (abs < 200) in blue and the \"outliers\" in red. Overlay this figure with the linear fit to only the inliers\n",
        "1. Make a third figure. Use the `fitLineHuber` function to estimate the model parameters and identify inliers and outliers. As before, plot the inliers in blue, the outliers in red, and overlay the figure with the linear fit to only the inliers\n",
        "1. Make a fourth figure that's the same as the third, except use the `fitLineRanac` function. \n",
        "1. Print the slope and intercept and their uncertainties for each method you used. \n",
        "\n",
        "\n",
        "Repeat these for different values of `deltat` Which time interval gives you the lowest uncertainty estimate of the slope and the intercept? Does which `deltat` is best depend on how you treat the outliers?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqGu67kRhFv_"
      },
      "source": [
        "## your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni8VhjpohEc-"
      },
      "source": [
        "#Questions\n",
        "\n",
        "1. To what extent are your data consistent with the theory? Do you see qualitative agreement (i.e. the direction matches the predicted direction and depends on the size of $ab/L^2$)? Is the measured rate linear in $ab/L^2$ ? Are the slope and intercepts what you would expect?\n",
        "\n",
        "1. Do the outliers identified by the algorithms make sense to you? The fitLineHuber takes an optional argument (epsilon=...) that sets how strictly outliers are excluded. I've set the default to 2. Try 1.35 (the toolkit default). Are most of the excluded data truly outliers? If your goal is to make an accurate estimate of the slope, what are the tradeoffs of excluding more outlying data?\n",
        "\n",
        "1. Which time interval gives you the lowest uncertainty estimate of the slope and the intercept? Does which deltat is best depend on how you treat the outliers?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_SfaQ9ciCv_"
      },
      "source": [
        "#Slice and dice the dataset\n",
        "\n",
        "Select a `deltat` you think gives the most accurate accounting of the motion. Please make the scatter plots of measured vs. predicted rates again (please continue to scale by degrees per hour). \n",
        "\n",
        "1. In one plot, please color the data according to the section number\n",
        "2. In a second plot, please color the data according to the setup number\n",
        "3. In a third plot, please color the data by whether the motion was parallel to the wall. Consider the motion to be parallel if it is within 45 degrees of the angle of the wall. If theta is the angle of the swing, then this code would be true for motion parallel to the wall `np.abs(np.cos(theta - np.deg2rad(data.thetaWall))) < np.sqrt(0.5)`\n",
        "\n",
        "Once you've sliced and diced for plotting, please find the slope and intercepts for each section, each setup, and parallel/away from the wall using a method for handling outliers you select. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdOyfR8lgPOv"
      },
      "source": [
        "#your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}